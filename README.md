Modelos de aprendizado de máquina têm sido utilizados extensivamente em diversas áreas do conhecimento e possuem inúmeras aplicações em quase todos os segmentos da atividade humana. Na área da saúde, o uso de técnicas de inteligência artificial tem revolucionado o diagnóstico de doenças com excelentes desempenhos na classificação de imagens. Embora esses modelos tenham alcançado resultados extraordinários, a falta de explicabilidade das 
decisões tomadas pelos modelos tem sido uma limitação significativa para a adoção generalizada dessas técnicas na prática clínica. 

O glaucoma é uma doença ocular neurodegenerativa que pode levar à cegueira de forma irreversível. A sua detecção precoce é crucial para prevenir a perda de visão. A detecção automatizada do glaucoma tem sido objeto de intensa pesquisa em visão computacional com diversos estudos propondo o uso de redes neurais convolucionais (CNNs) para analisar automaticamente imagens de fundo de retina e diagnosticar a doença. No entanto, essas propostas carecem de explicabilidade, o que é crucial para que os oftalmologistas compreendam as decisões tomadas pelos modelos e possam justificá-las aos seus pacientes. 

Este trabalho tem a finalidade de explorar e aplicar técnicas de inteligência artificial explicável (XAI) em diferentes arquiteturas de CNNs para a classificação do glaucoma e fazer uma análise comparativa sobre quais métodos de explicação fornecem os melhores recursos para a interpretação humana, servindo de apoio no diagnóstico clínico.
